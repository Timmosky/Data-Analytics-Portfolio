parts_WeekLevel = 
ADDCOLUMNS(
    SUMMARIZE(
        parts,
        parts[Order Fiscal Week],
        parts[Item Description],
        parts[Ultimate_family_name]
        /* Remove Order Year and numericquarter from SUMMARIZE */
    ),
    "Order Year",
        VAR CurWeek = [Order Fiscal Week]
        RETURN INT(CurWeek / 100),
    
    "numericquarter",
        VAR CurWeek = [Order Fiscal Week]
        VAR WeekNum = MOD(CurWeek, 100)
        RETURN 
        SWITCH(
            TRUE(),
            WeekNum >= 1 && WeekNum <= 13, 1,
            WeekNum >= 14 && WeekNum <= 26, 2,
            WeekNum >= 27 && WeekNum <= 39, 3,
            WeekNum >= 40 && WeekNum <= 53, 4,
            BLANK()
        ),
    
    "Quantity", 
        VAR CurWeek = [Order Fiscal Week]
        VAR CurItem = [Item Description]
        VAR CurFamily = [Ultimate_family_name]
        RETURN
        CALCULATE(
            SUM(parts[Quantity]),
            parts[Order Fiscal Week] = CurWeek,
            parts[Item Description] = CurItem,
            parts[Ultimate_family_name] = CurFamily
        ),
    
    "System_Count",
        VAR CurWeek = [Order Fiscal Week]
        VAR CurItem = [Item Description]
        VAR CurFamily = [Ultimate_family_name]
        RETURN
        CALCULATE(
            /* Use MAX or AVERAGE depending on your logic */
            MAX(parts[System_Count_Column]),
            parts[Order Fiscal Week] = CurWeek,
            parts[Item Description] = CurItem,
            parts[Ultimate_family_name] = CurFamily
        ),
    
    "systemid",
        VAR CurWeek = [Order Fiscal Week]
        VAR CurItem = [Item Description]
        VAR CurFamily = [Ultimate_family_name]
        RETURN
        CONCATENATEX(
            FILTER(
                parts,
                parts[Order Fiscal Week] = CurWeek
                && parts[Item Description] = CurItem
                && parts[Ultimate_family_name] = CurFamily
            ),
            parts[systemid],
            ", ",
            parts[systemid], ASC
        ),
    
    "item_key",
        VAR CurItem = [Item Description]
        RETURN
        CONCATENATEX(
            DISTINCT(
                SELECTCOLUMNS(
                    FILTER(
                        parts,
                        parts[Item Description] = CurItem
                    ),
                    "ItemKey", parts[item_key]
                )
            ),
            [ItemKey],
            ", ",
            [ItemKey], ASC
        ),
    
    "Order_Date_Max",
        VAR CurWeek = [Order Fiscal Week]
        VAR CurItem = [Item Description]
        VAR CurFamily = [Ultimate_family_name]
        RETURN
        CALCULATE(
            MAX(parts[Order Date]),
            parts[Order Fiscal Week] = CurWeek,
            parts[Item Description] = CurItem,
            parts[Ultimate_family_name] = CurFamily
        )
)



















ðŸ› ï¸ Step-by-Step Implementation
Step 1: âœ… Store Service Records in S3
Example file: s3://your-bucket/service-records/service_records.csv

Format: CSV with fields:

product_id

timestamp (date of usage/service)

utilization_metric (e.g., usage hours, count)

Step 2: âœ… Write the Detection Script (detect_anomalies.py)
This runs inside SageMaker Processing:

python
Copy
Edit
# detect_anomalies.py
import pandas as pd
from sklearn.ensemble import IsolationForest
import argparse
import os

def detect_anomalies(df):
    all_anomalies = []

    for product_id, group in df.groupby('product_id'):
        usage = (
            group.set_index('timestamp')
            .resample('W')['utilization_metric']
            .sum()
            .fillna(0)
            .reset_index()
        )

        if len(usage) < 10:
            continue

        model = IsolationForest(contamination=0.1, random_state=42)
        usage['anomaly'] = model.fit_predict(usage[['utilization_metric']])
        usage['anomaly'] = usage['anomaly'].map({1: 0, -1: 1})
        usage['product_id'] = product_id

        anomalies = usage[usage['anomaly'] == 1]
        all_anomalies.append(anomalies)

    if all_anomalies:
        result = pd.concat(all_anomalies, ignore_index=True)
        return result
    return pd.DataFrame()

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--input-data', type=str)
    parser.add_argument('--output-data', type=str)
    args = parser.parse_args()

    df = pd.read_csv(os.path.join(args.input_data, 'service_records.csv'), parse_dates=['timestamp'])
    anomalies = detect_anomalies(df)
    anomalies.to_csv(os.path.join(args.output_data, 'anomaly_report.csv'), index=False)
Step 3: âœ… Create SageMaker Processing Job
python
Copy
Edit
import sagemaker
from sagemaker.sklearn.processing import SKLearnProcessor
from sagemaker.processing import ProcessingInput, ProcessingOutput

role = 'arn:aws:iam::<your-account-id>:role/service-role/sagemaker-execution-role'  # Adjust as needed

sklearn_processor = SKLearnProcessor(
    framework_version='0.23-1',
    role=role,
    instance_type='ml.m5.large',
    instance_count=1,
    base_job_name='utilization-anomaly-detection'
)

sklearn_processor.run(
    code='detect_anomalies.py',
    inputs=[
        ProcessingInput(
            source='s3://your-bucket/service-records/',
            destination='/opt/ml/processing/input'
        )
    ],
    outputs=[
        ProcessingOutput(
            source='/opt/ml/processing/output',
            destination='s3://your-bucket/anomaly-results/'
        )
    ],
    arguments=[
        '--input-data', '/opt/ml/processing/input',
        '--output-data', '/opt/ml/processing/output'
    ]
)
Step 4: âœ… Send Alerts via SNS (Optional Lambda)
Create a Lambda function that:

Gets anomaly_report.csv from S3

Parses it

Sends a summary email via SNS

python
Copy
Edit
import boto3
import pandas as pd
import io

def lambda_handler(event, context):
    s3 = boto3.client('s3')
    sns = boto3.client('sns')

    BUCKET = 'your-bucket'
    KEY = 'anomaly-results/anomaly_report.csv'
    SNS_TOPIC = 'arn:aws:sns:your-region:your-topic'

    try:
        obj = s3.get_object(Bucket=BUCKET, Key=KEY)
        df = pd.read_csv(io.BytesIO(obj['Body'].read()))
        
        if not df.empty:
            msg = '\n'.join([
                f"Product: {row['product_id']} | Date: {row['timestamp']} | Utilization: {row['utilization_metric']}"
                for _, row in df.iterrows()
            ])
            sns.publish(
                TopicArn=SNS_TOPIC,
                Subject='âš ï¸ Anomalies Detected in Product Utilization',
                Message=msg
            )
    except Exception as e:
        print("Error processing anomaly report:", e)
Schedule this Lambda with EventBridge to run shortly after the SageMaker job completes.

Step 5: âœ… (Optional) Automate with EventBridge or Step Functions
You can schedule or automate:

Triggering the SageMaker Processing job

Invoking the Lambda after it finishes

Use Step Functions to link the two with a visual workflow if needed.

ðŸ§¾ Summary
Task	AWS Service
Store service records	Amazon S3
Run anomaly detection	SageMaker Processing
Schedule or orchestrate	EventBridge / Step Functions
Send alerts	Lambda + SNS
View or analyze results	Athena, QuickSight, or download














First 8W Exceed Quarter :=
VAR CurrItem  = SELECTEDVALUE ( parts[Item Description] )
VAR CurrYear  = SELECTEDVALUE ( 'DateTable'[FiscalYear] )

RETURN
MINX (
    FILTER (
        ADDCOLUMNS (
            SUMMARIZE (
                ALL ( parts ),
                parts[Item Description],
                'DateTable'[FiscalYear],
                'DateTable'[FiscalQuarter]
            ),
            "EP8", CALCULATE ( [Exceedance_8W] )
        ),
        [Item Description] = CurrItem
            && 'DateTable'[FiscalYear] = CurrYear
            && [EP8] >= 2              -- 2+ consecutive weeks
    ),
    'DateTable'[FiscalQuarter]
)
----------------------------------------------------------------------------

Combined Anomaly Category :=
VAR CurrItem     = SELECTEDVALUE ( parts[Item Description] )
VAR CurrYear     = SELECTEDVALUE ( 'DateTable'[FiscalYear] )
VAR CurrQuarter  = SELECTEDVALUE ( 'DateTable'[FiscalQuarter] )

VAR Ex8          = [Exceedance_8W]
VAR ExQ          = [Exceedance_QTR]

VAR First8Q      = [First 8W Exceed Quarter]

-- Is a sustained anomaly allowed here?
VAR SustainedAllowed =
    NOT ISBLANK ( First8Q )             -- 8W anomaly happened this year
        && CurrQuarter >= First8Q       -- not before the first 8W spike
        && CurrQuarter <= First8Q + 1   -- at most 1 quarter later

RETURN
SWITCH (
    TRUE (),

    -- 1) Sustained anomaly (quarterly) â€“ only if allowed by 8W rule
    ExQ >= 2 && SustainedAllowed, "Sustained anomaly detected",

    -- 2) Plain anomaly from 8-week logic
    Ex8 >= 2,                           "Anomaly detected",

    -- 3) No flag
    BLANK ()
)

------------------------------------
First 8W Exceed Quarter :=
VAR CurrItem =
    SELECTEDVALUE(parts[Item Description])

VAR CurrYear =
    SELECTEDVALUE(parts[Order Fiscal Year])

RETURN
MINX(
    FILTER(
        ADDCOLUMNS(
            SUMMARIZE(
                FILTER(
                    ALLSELECTED(parts),
                    parts[Item Description] = CurrItem &&
                    parts[Order Fiscal Year] = CurrYear
                ),
                parts[Item Description],
                parts[Order Fiscal Year],
                parts[FQ_Num]
            ),
            "EP8", CALCULATE([Exceedance_8W])
        ),
        [EP8] >= 2
    ),
    parts[FQ_Num]
)
--------------------------------
TrailingAnomaly_Lagged_Filtered = 
VAR CurrentRate = parts_WeekLevel[Normal_Rate]
VAR UpperLimit = parts_WeekLevel[TrailingUpperLimit_Lagged]
VAR MinimumRateThreshold = 0.00015  /* 1.5 parts per 10,000 systems */

RETURN
IF(
    NOT ISBLANK(CurrentRate) 
    && NOT ISBLANK(UpperLimit)
    && CurrentRate > UpperLimit 
    && CurrentRate >= MinimumRateThreshold,
    1,
    0
)
